{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL to fetch\n",
    "url = \"https://www.carbonbrief.org/daily-brief/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Failed to fetch URL: {url} (status code: {response.status_code})\")\n",
    "\n",
    "# Get the HTML content from the response\n",
    "html_content = response.text\n",
    "\n",
    "# Ensure the 'temp' directory exists\n",
    "os.makedirs(\"temp\", exist_ok=True)\n",
    "\n",
    "# Save the raw HTML content to a file\n",
    "html_file_path = os.path.join(\"temp\", \"daily_brief.html\")\n",
    "with open(html_file_path, \"w\", encoding=\"utf-8\") as html_file:\n",
    "    html_file.write(html_content)\n",
    "print(f\"HTML content saved to: {html_file_path}\")\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup for article extraction\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find all article elements on the page.\n",
    "# (This selector may need to be updated if the website structure changes.)\n",
    "articles = soup.find_all(\"article\")\n",
    "\n",
    "output_lines = []\n",
    "\n",
    "if articles:\n",
    "    for article in articles:\n",
    "        # Extract the title from header tags (commonly h1, h2, or h3)\n",
    "        title_element = article.find([\"h1\", \"h2\", \"h3\"])\n",
    "        title = title_element.get_text(strip=True) if title_element else \"No Title Found\"\n",
    "\n",
    "        # Try to get the URL from the title link, if available\n",
    "        link = \"\"\n",
    "        if title_element:\n",
    "            a_tag = title_element.find(\"a\")\n",
    "            if a_tag and a_tag.has_attr(\"href\"):\n",
    "                link = a_tag[\"href\"]\n",
    "\n",
    "        # Extract a brief excerpt if available (e.g., the first paragraph)\n",
    "        excerpt_element = article.find(\"p\")\n",
    "        excerpt = excerpt_element.get_text(strip=True) if excerpt_element else \"No excerpt available.\"\n",
    "\n",
    "        # Build the output for this article\n",
    "        output_lines.append(f\"Title: {title}\")\n",
    "        if link:\n",
    "            output_lines.append(f\"Link: {link}\")\n",
    "        output_lines.append(f\"Excerpt: {excerpt}\")\n",
    "        output_lines.append(\"-\" * 80)\n",
    "else:\n",
    "    # If no article elements are found, fall back to saving the entire page text.\n",
    "    output_lines.append(\"No specific article blocks were found. Here is the entire page text:\")\n",
    "    output_lines.append(soup.get_text(separator=\"\\n\", strip=True))\n",
    "\n",
    "# Join the lines into a single string\n",
    "output_text = \"\\n\".join(output_lines)\n",
    "\n",
    "# Save the extracted article information to a text file\n",
    "text_file_path = os.path.join(\"temp\", \"daily_brief.txt\")\n",
    "with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(output_text)\n",
    "\n",
    "print(f\"Article information saved to: {text_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input HTML file path (from the previous step)\n",
    "input_file_path = os.path.join(\"temp\", \"daily_brief.html\")\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Extract all <div> elements with class \"dailystory\"\n",
    "dailystory_divs = soup.find_all(\"div\", class_=\"dailystory\")\n",
    "\n",
    "# Extract all <div> elements with class \"breadCrumbNormal\" that include the text \"Daily Briefing\"\n",
    "breadCrumb_divs = []\n",
    "for div in soup.find_all(\"div\", class_=\"breadCrumbNormal\"):\n",
    "    if \"Daily Briefing\" in div.get_text():\n",
    "        breadCrumb_divs.append(div)\n",
    "\n",
    "# Build a new HTML document that includes only the filtered divs\n",
    "filtered_html = \"<html>\\n\"\n",
    "filtered_html += \"<head>\\n\"\n",
    "filtered_html += \"  <meta charset='utf-8'>\\n\"\n",
    "filtered_html += \"  <title>Filtered Daily Brief</title>\\n\"\n",
    "filtered_html += \"</head>\\n\"\n",
    "filtered_html += \"<body>\\n\"\n",
    "\n",
    "# Append the breadCrumb divs first (if any)\n",
    "for div in breadCrumb_divs:\n",
    "    filtered_html += str(div) + \"\\n\"\n",
    "\n",
    "# Append the dailystory divs\n",
    "for div in dailystory_divs:\n",
    "    filtered_html += str(div) + \"\\n\"\n",
    "\n",
    "filtered_html += \"</body>\\n</html>\"\n",
    "\n",
    "# Define the output HTML file path\n",
    "output_file_path = os.path.join(\"temp\", \"filtered_daily_brief.html\")\n",
    "\n",
    "# Write the filtered HTML content to the output file\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    outfile.write(filtered_html)\n",
    "\n",
    "print(f\"Filtered HTML saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input/output HTML file path\n",
    "file_path = os.path.join(\"temp\", \"filtered_daily_brief.html\")\n",
    "\n",
    "# Read the existing HTML content\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Remove the <a> tags but keep their inner text by unwrapping them\n",
    "for a_tag in soup.find_all(\"a\"):\n",
    "    a_tag.unwrap()\n",
    "\n",
    "# Remove all <span> formatting by unwrapping them (keeping their inner text)\n",
    "for span in soup.find_all(\"span\"):\n",
    "    span.unwrap()\n",
    "\n",
    "# Write the modified HTML back to the same file\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(str(soup))\n",
    "\n",
    "print(f\"All <a> tags have been removed and <span> tags have been unwrapped in {file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input HTML file path (adjust if needed)\n",
    "input_file_path = os.path.join(\"temp\", \"filtered_daily_brief.html\")\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find the first <div> with class \"dateCat\"\n",
    "date_cat_div = soup.find(\"div\", class_=\"dateCat\")\n",
    "\n",
    "# Extract the text from the div or use a fallback if not found\n",
    "date_text = date_cat_div.get_text(strip=True) if date_cat_div else \"an unknown date\"\n",
    "\n",
    "# Create the formatted output text\n",
    "output_text = f\"This is your Daily Carbon Brief, published on {date_text}\"\n",
    "\n",
    "# Define the output directory path\n",
    "output_dir = os.path.join(\"temp\", \"daily_brief_components\")\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = os.path.join(output_dir, \"001.txt\")\n",
    "\n",
    "# Write the output text to the file\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    outfile.write(output_text)\n",
    "\n",
    "print(f\"Output saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input HTML file path\n",
    "input_file_path = os.path.join(\"temp\", \"filtered_daily_brief.html\")\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find all <div> elements with class \"dailystory\"\n",
    "dailystory_divs = soup.find_all(\"div\", class_=\"dailystory\")\n",
    "\n",
    "# Define the output directory and ensure it exists\n",
    "output_dir = os.path.join(\"temp\", \"daily_brief_components\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# File numbering starts at 2 because 001.txt was used for the date component.\n",
    "file_number = 2\n",
    "\n",
    "# Process each dailystory div\n",
    "for story_div in dailystory_divs:\n",
    "    output_lines = []\n",
    "\n",
    "    # 1. Extract and add the story heading from <strong class=\"storyheading\">\n",
    "    heading_tag = story_div.find(\"strong\", class_=\"storyheading\")\n",
    "    heading_text = heading_tag.get_text(strip=True) + \".\" if heading_tag else \"No Heading Found\"\n",
    "    output_lines.append(heading_text)\n",
    "\n",
    "    # 2. Extract and process the story credits from <div class=\"storycredits\">\n",
    "    credits_tag = story_div.find(\"div\", class_=\"storycredits\")\n",
    "    if credits_tag:\n",
    "        # Get the full text and remove \"Read Article\"\n",
    "        credits_text = credits_tag.get_text(strip=True).replace(\"Read Article\", \"\").strip()\n",
    "        if credits_text:\n",
    "            # Format the credits as \"Published in {agency}\"\n",
    "            output_lines.append(f\"Published by {credits_text}.\")\n",
    "\n",
    "    # 3. Extract the story content from <div class=\"storycont\">\n",
    "    story_cont_div = story_div.find(\"div\", class_=\"storycont\")\n",
    "    if story_cont_div:\n",
    "        # Find all <p> tags inside the story content div\n",
    "        paragraphs = story_cont_div.find_all(\"p\")\n",
    "        # Extract each paragraph's text, stripping any extra whitespace\n",
    "        paragraph_texts = [p.get_text(strip=True) for p in paragraphs]\n",
    "        # Join the paragraphs with a newline between each\n",
    "        output_lines.append(\"\\n\".join(paragraph_texts))\n",
    "\n",
    "    # Combine all parts with double newlines separating sections\n",
    "    output_text = \"\\n\\n\".join(output_lines)\n",
    "\n",
    "    # Create a sequential filename (e.g., \"002.txt\", \"003.txt\", etc.)\n",
    "    file_name = f\"{file_number:03d}.txt\"\n",
    "    output_file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Write the output text to the file\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        out_file.write(output_text)\n",
    "\n",
    "    print(f\"Story saved to: {output_file_path}\")\n",
    "\n",
    "    file_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the component files\n",
    "components_dir = os.path.join(\"temp\", \"daily_brief_components\")\n",
    "# Define the output file path\n",
    "output_file_path = os.path.join(\"temp\", \"daily_brief.txt\")\n",
    "\n",
    "# Get a list of all .txt files in the components directory and sort them (alphabetical sorting works if they are numbered with leading zeros)\n",
    "component_files = [f for f in os.listdir(components_dir) if f.endswith(\".txt\")]\n",
    "component_files.sort()\n",
    "\n",
    "# Initialize a list to hold the content of each component file\n",
    "all_components = []\n",
    "\n",
    "# Read each component file in order\n",
    "for file_name in component_files:\n",
    "    file_path = os.path.join(components_dir, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read().strip()\n",
    "        all_components.append(content)\n",
    "\n",
    "# Join all the component texts with two newlines between each\n",
    "daily_brief_text = \"\\n\\n\".join(all_components)\n",
    "\n",
    "# Write the combined content to daily_brief.txt\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(daily_brief_text)\n",
    "\n",
    "print(f\"Combined daily brief saved to: {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
